{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOPIC NAME\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This guide will show you how to:\n",
    "\n",
    "* TODO\n",
    "* TODO\n",
    "\n",
    "By the end of it, you will TODO!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install neptune-client TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: TODO\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "\n",
    "from ignite.contrib.handlers.neptune_logger import *\n",
    "\n",
    "LOG_INTERVAL = 10\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "def get_data_loaders(train_batch_size, val_batch_size):\n",
    "    data_transform = Compose([ToTensor(), Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "    train_loader = DataLoader(MNIST(download=True, root=\".\", transform=data_transform, train=True),\n",
    "                              batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "    val_loader = DataLoader(MNIST(download=False, root=\".\", transform=data_transform, train=False),\n",
    "                            batch_size=val_batch_size, shuffle=False)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "train_batch_size = 64\n",
    "val_batch_size = 1000\n",
    "epochs = 10\n",
    "lr = 0.01\n",
    "momentum = 0.5\n",
    "\n",
    "train_loader, val_loader = get_data_loaders(train_batch_size, val_batch_size)\n",
    "model = Net()\n",
    "device = 'cpu'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "\n",
    "optimizer = SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "trainer = create_supervised_trainer(model, optimizer, criterion, device=device)\n",
    "\n",
    "metrics = {\n",
    "    'accuracy': Accuracy(),\n",
    "    'loss': Loss(criterion)\n",
    "}\n",
    "\n",
    "train_evaluator = create_supervised_evaluator(model, metrics=metrics, device=device)\n",
    "validation_evaluator = create_supervised_evaluator(model, metrics=metrics, device=device)\n",
    "\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def compute_metrics(engine):\n",
    "    train_evaluator.run(train_loader)\n",
    "    validation_evaluator.run(val_loader)\n",
    "\n",
    "\n",
    "npt_logger = NeptuneLogger(api_token=None,\n",
    "                           project_name=\"neptune-ai/pytorch-ignite-integration\",\n",
    "                           name='ignite-mnist-example',\n",
    "                           params={'train_batch_size': train_batch_size,\n",
    "                                   'val_batch_size': val_batch_size,\n",
    "                                   'epochs': epochs,\n",
    "                                   'lr': lr,\n",
    "                                   'momentum': momentum})\n",
    "\n",
    "npt_logger.attach(trainer,\n",
    "                  log_handler=OutputHandler(tag=\"training\",\n",
    "                                            output_transform=lambda loss: {'batchloss': loss},\n",
    "                                            metric_names='all'),\n",
    "                  event_name=Events.ITERATION_COMPLETED(every=100))\n",
    "\n",
    "npt_logger.attach(train_evaluator,\n",
    "                  log_handler=OutputHandler(tag=\"training\",\n",
    "                                            metric_names=[\"loss\", \"accuracy\"],\n",
    "                                            another_engine=trainer),\n",
    "                  event_name=Events.EPOCH_COMPLETED)\n",
    "\n",
    "npt_logger.attach(validation_evaluator,\n",
    "                  log_handler=OutputHandler(tag=\"validation\",\n",
    "                                            metric_names=[\"loss\", \"accuracy\"],\n",
    "                                            another_engine=trainer),\n",
    "                  event_name=Events.EPOCH_COMPLETED)\n",
    "\n",
    "npt_logger.attach(trainer,\n",
    "                  log_handler=OptimizerParamsHandler(optimizer),\n",
    "                  event_name=Events.ITERATION_COMPLETED(every=100))\n",
    "\n",
    "npt_logger.attach(trainer,\n",
    "                  log_handler=WeightsScalarHandler(model),\n",
    "                  event_name=Events.ITERATION_COMPLETED(every=100))\n",
    "\n",
    "npt_logger.attach(trainer,\n",
    "                  log_handler=GradsScalarHandler(model),\n",
    "                  event_name=Events.ITERATION_COMPLETED(every=100))\n",
    "\n",
    "# kick everything off\n",
    "trainer.run(train_loader, max_epochs=epochs)\n",
    "\n",
    "# log additional information\n",
    "torch.save(model.state_dict(), 'model.pth')\n",
    "npt_logger.experiment.log_artifact('model.pth')\n",
    "\n",
    "npt_logger.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Initialize Neptune\n",
    "\n",
    "Connects your script to Neptune application. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune\n",
    "\n",
    "neptune.init(\n",
    "    api_token=\"ANONYMOUS\",\n",
    "    project_qualified_name=\"shared/onboarding\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You tell Neptune: \n",
    "\n",
    "* **who you are**: your Neptune API token `api_token` \n",
    "* **where you want to send your data**: your Neptune project `project_qualified_name`.\n",
    "\n",
    "---\n",
    "\n",
    "**Note:** \n",
    "\n",
    "\n",
    "Instead of logging data to the public project 'shared/onboarding' as an anonymous user 'neptuner' you can log it to your own project.\n",
    "\n",
    "To do that:\n",
    "\n",
    "1. Get your Neptune API token\n",
    "\n",
    "![image](https://neptune.ai/wp-content/uploads/get_token.gif)\n",
    "\n",
    "2. Pass the token to ``api_token`` argument of ``neptune.init()`` method: ``api_token=YOUR_API_TOKEN``\n",
    "3. Pass your username to the ``project_qualified_name`` argument of the ``neptune.init()`` method: ``project_qualified_name='YOUR_USERNAME/sandbox``. Keep `/sandbox` at the end, the `sandbox` project that was automatically created for you.\n",
    "\n",
    "For example:\n",
    "\n",
    "```python\n",
    "neptune.init(project_qualified_name='funky_steve/sandbox', \n",
    "             api_token='eyJhcGlfYW908fsdf23f940jiri0bn3085gh03riv03irn',\n",
    "            )\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neptune.create_experiment(name='great-idea')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This opens a new \"experiment\" namespace in Neptune to which you can log various objects.\n",
    "\n",
    "Click on the link above to open this experiment in Neptune.\n",
    "\n",
    "For now it is empty but keep the tab with experiment open to see what happens next. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: TODO\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Note:**\n",
    "   \n",
    "When you track experiments with Neptune in Jupyter notebooks you need to explicitly stop the experiment by running `neptune.stop()`.\n",
    "\n",
    "If you are running Neptune in regular `.py` scripts it will stop automatically when your code stops running.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neptune.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_37",
   "language": "python",
   "name": "py_37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
